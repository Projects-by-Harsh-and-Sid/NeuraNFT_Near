{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id_='0857b3fa-8523-4730-92a3-dc2a74475c0c', embedding=None, metadata={'page_label': '1', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Flask_Server\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-09', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='How Good Are Low-bit Quantized LLAMA3 Models?\\nAn Empirical Study\\nWei Huang∗\\nThe University of Hong Kong\\nweih@connect.hku.hkXudong Ma∗\\nBeihang University\\nmacaronlin@buaa.edu.cn\\nHaotong Qin†\\nETH Zurich\\nhaotong.qin@pbl.ee.ethz.chXingyu Zheng\\nBeihang University\\nxingyuzheng@buaa.edu.cn\\nChengtao Lv\\nBeihang University\\nlvchengtao@buaa.edu.cnHong Chen\\nBeihang University\\n18373205@buaa.edu.cnJie Luo\\nBeihang University\\nluojie@buaa.edu.cn\\nXiaojuan Qi\\nThe University of Hong Kong\\nxjqi@eee.hku.hkXianglong Liu\\nBeihang University\\nxlliu@buaa.edu.cnMichele Magno\\nETH Zurich\\nmichele.magno@pbl.ee.ethz.ch\\nAbstract\\nMeta’s LLAMA family has become one of the most powerful open-source Large\\nLanguage Model (LLM) series. Notably, LLAMA3 models have recently been\\nreleased and achieve impressive performance across various with super-large scale\\npre-training on over 15T tokens of data. Given the wide application of low-\\nbit quantization for LLMs in resource-limited scenarios, we explore LLAMA3 ’s\\ncapabilities when quantized to low bit-width. This exploration holds the potential to\\nunveil new insights and challenges for low-bit quantization of LLAMA3 and other\\nforthcoming LLMs, especially in addressing performance degradation problems\\nthat suffer in LLM compression. Specifically, we evaluate the 10 existing post-\\ntraining quantization and LoRA-finetuning methods of LLAMA3 on 1-8 bits\\nand diverse datasets to comprehensively reveal LLAMA3 ’s low-bit quantization\\nperformance. Our experiment results indicate that LLAMA3 still suffers non-\\nnegligent degradation in these scenarios, especially in ultra-low bit-width. This\\nhighlights the significant performance gap under low bit-width that needs to be\\nbridged in future developments. We expect that this empirical study will prove\\nvaluable in advancing future models, pushing the LLMs to lower bit-width with\\nhigher accuracy for being practical. Our project is released on https://github.\\ncom/Macaronlin/LLaMA3-Quantization and quantized LLAMA3 models are\\nreleased in https://huggingface.co/LLMQ .\\n1 Introduction\\nLaunched by Meta in February 2023, the LLaMA [ 18] series2represents a breakthrough in autore-\\ngressive large language models (LLMs) using the Transformer [ 19] architecture. Right from its first\\n∗Equal Contribution.†Corresponding Author.\\n2https://llama.meta.comarXiv:2404.14047v1  [cs.LG]  22 Apr 2024', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='13e77c14-9716-4237-b63c-45874d6dce24', embedding=None, metadata={'page_label': '2', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Flask_Server\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-09', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Evaluated LLMsLLaMA3-8BLLaMA3-70B1QuantizationMethodsRTN2GPTQAWQSmoothQuantPB-LLMBiLLMQuIPDB-LLMQLoRAIR-QLoRAEvaluationDatasetsWikiText2C4PTBPIQAARC-eARC-cHellaSwag3WinograndePerplexity↓CommonSenseQA↑Post-Training QuantizationLoRA-FinetuningHumanitiesSTEMSocialOtherMMLU↑Figure 1: The overview of our empirical study\\nversion, with 13 billion parameters, it managed to outperform the much larger, closed-source GPT-3\\nmodel which boasts 175 billion parameters. On April 18, 2024, Meta introduced the LLAMA3\\nmodel, offering configurations of 8 billion and 70 billion parameters. Thanks to extensive pre-training\\non more than 15 trillion data tokens, the LLAMA3 models3have achieved state-of-the-art (SOTA)\\nperformance across a broad range of tasks, establishing the LLaMA family as among the finest\\nopen-source LLMs available for a wide variety of applications and deployment scenarios.\\nDespite their impressive performance, deploying LLAMA3 models still poses significant challenges\\ndue to resource limitations in many scenarios. Fortunately, low-bit quantization has emerged as one\\nof the most popular techniques for compressing LLMs. This technique reduces the memory and\\ncomputational requirements of LLMs during inference, enabling them to run on resource-limited\\ndevices. Addressing the performance drop that occurs after compression is a major concern for\\ncurrent LLM quantization approaches. While numerous low-bit quantization methods have been\\nproposed, their evaluations have primarily focused on the earlier and less capable LLaMA models\\n(LLAMA1 andLLAMA2 ). Thus, LLAMA3 presents a new opportunity for the LLM community\\nto assess the performance of quantization on cutting-edge LLMs and to understand the strengths\\nand limitations of existing methods. In this empirical study, our aim is to analyze the capability of\\nLLAMA3 to handle the challenges associated with degradation due to quantization.\\nOur study sets out two primary technology tracks for quantizing LLMs: Post-Training Quantization\\n(PTQ) and LoRA-FineTuning (LoRA-FT) quantization, with the aim of providing a comprehensive\\nevaluation of the LLAMA3 models’ quantization. We explore a range of cutting-edge quantization\\nmethods across technical tracks (RTN, GPTQ [ 6], AWQ [ 10], SmoothQuant [ 20], PB-LLM [ 16],\\nQuIP [ 2], DB-LLM [ 3], and BiLLM [ 9] for PTQ; QLoRA [ 5] and IR-QLoRA [ 13] for LoRA-FT),\\ncovering a wide spectrum from 1 to 8 bits and utilizing a diverse array of evaluation datasets, including\\nWikiText2, C4, PTB, CommonSenseQA datasets (PIQA, ARC-e, ARC-c, HellaSwag, Winogrande),\\nand MMLU benchmark. The overview of our study is presented as Figure 1. These evaluations\\nassess the capabilities and limits of the LLAMA3 model under current LLM quantization techniques\\nand serve as a source of inspiration for the design of future LLM quantization methods. The choice\\nto focus specifically on the LLAMA3 model is motivated by its superior performance among all\\ncurrent open-source instruction-tuned LLMs across a variety of datasets3, including 5-shot MMLU,\\n0-shot GPQA, 0-shot HumanEval, 8-shot CoT GSM-8K, and 4-shot CoT MATH. Furthermore, we\\nhave made our project and the quantized models available to the public on https://github.com/\\nMacaronlin/LLaMA3-Quantization andhttps://huggingface.co/LLMQ , respectively. This\\nnot only aids in advancing the research within the LLM quantization community but also facilitates a\\nbroader understanding and application of effective quantization techniques.\\n2 Empirical Evaluation\\n2.1 Experiment Settings\\nEvaluated LLMs. We obtain the pre-trained LLAMA3 -8B and -70B through the official repository3.\\nQuantization methods. To evaluate the performance of low-bit quantized LLAMA3 , we select\\nrepresentative LLM quantization methods with extensive influence and functionality, including 8\\n3https://github.com/meta-llama/llama3\\n2', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='afe6794c-17fb-412e-864b-d6dea79c137b', embedding=None, metadata={'page_label': '3', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Flask_Server\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-09', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 1: Evaluation results of post-training quantization on LL AMA3-8B model\\nMethod #W #A #GPPL↓ CommonSenseQA ↑\\nWikiText2 C4 PTB PIQA ARC-e ARC-c HellaSwag Wino Avg.\\nLLAMA3 16 16 - 6.1 9.2 10.6 79.9 80.1 50.4 60.2 72.8 68.6\\nRTN4 16 128 8.5 13.4 14.5 76.6 70.1 45.0 56.8 71.0 63.9\\n3 16 128 27.9 1.1e2 95.6 62.3 32.1 22.5 29.1 54.7 40.2\\n2 16 128 1.9E3 2.5E4 1.8E4 53.1 24.8 22.1 26.9 53.1 36.0\\n8 16 - 6.2 9.5 11.2 79.7 80.8 50.4 60.1 73.4 68.9\\n4 16 - 8.7 14.0 14.9 75.0 68.2 39.4 56.0 69.0 61.5\\n3 16 - 2.2E3 5.6E2 2.0E3 56.2 31.1 20.0 27.5 53.1 35.6\\n2 16 - 2.7E6 7.4E6 3.1E6 53.1 24.7 21.9 25.6 51.1 35.3\\nGPTQ4 16 128 6.5 10.4 11.0 78.4 78.8 47.7 59.0 72.6 67.3\\n3 16 128 8.2 13.7 15.2 74.9 70.5 37.7 54.3 71.1 61.7\\n2 16 128 2.1E2 4.1E4 9.1E2 53.9 28.8 19.9 27.7 50.5 36.2\\n8 16 - 6.1 9.4 10.6 79.8 80.1 50.2 60.2 72.8 68.6\\n4 16 - 7.0 11.8 14.4 76.8 74.3 42.4 57.4 72.8 64.8\\n3 16 - 13.0 45.9 37.0 60.8 38.8 22.3 41.8 60.9 44.9\\n2 16 - 5.7E4 1.0E5 2.7E5 52.8 25.0 20.5 26.6 49.6 34.9\\nAWQ4 16 128 6.6 9.4 11.1 79.1 79.7 49.3 59.1 74.0 68.2\\n3 16 128 8.2 11.6 13.2 77.7 74.0 43.2 55.1 72.1 64.4\\n2 16 128 1.7E6 2.1E6 1.8E6 52.4 24.2 21.5 25.6 50.7 34.9\\n8 16 - 6.1 8.9 10.6 79.6 80.3 50.5 60.2 72.8 68.7\\n4 16 - 7.1 10.1 11.8 78.3 77.6 48.3 58.6 72.5 67.0\\n3 16 - 12.8 16.8 24.0 71.9 66.7 35.1 50.7 64.7 57.8\\n2 16 - 8.2E5 8.1E5 9.0E5 55.2 25.2 21.3 25.4 50.4 35.5\\nQuIP4 16 - 6.5 11.1 9.5 78.2 78.2 47.4 58.6 73.2 67.1\\n3 16 - 7.5 11.3 12.6 76.8 72.9 41.0 55.4 72.5 63.7\\n2 16 - 85.1 1.3E2 1.8E2 52.9 29.0 21.3 29.2 51.7 36.8\\nDB-LLM 2 16 128 13.6 19.2 23.8 68.9 59.1 28.2 42.1 60.4 51.8\\nPB-LLM2 16 128 24.7 79.2 65.6 57.0 37.8 17.2 29.8 52.5 38.8\\n1.7 16 128 41.8 2.6E2 1.2E2 52.5 31.7 17.5 27.7 50.4 36.0\\nBiLLM 1.1 16 128 28.3 2.9E2 94.7 56.1 36.0 17.7 28.9 51.0 37.9\\nSmoothQuant8 8 - 6.3 9.2 10.8 79.5 79.7 49.0 60.0 73.2 68.3\\n6 6 - 7.7 11.8 12.5 76.8 75.5 45.0 56.9 69.0 64.6\\n4 4 - 4.3E3 4.0E3 3.6E3 54.6 26.3 20.0 26.4 50.3 35.5\\nPTQ methods and 2 LoRA-FT methods. The implementations of our evaluated quantization methods\\nfollow their open-source repositories4. We also used eight NVIDIA A800 with 80GB GPU memory\\nfor quantitative evaluation.\\nEvaluation datasets. For the PTQ methods, we evaluate quantized LLAMA3 on the WikiText2 [ 12],\\nPTB [ 11], and a portion of the C4 dataset [ 14], using Perplexity (PPL) as the evaluation metric.\\nSubsequently, we further conduct experiments on five zero-shot evaluation tasks (PIQA [ 1], Wino-\\ngrande [ 15], ARC-e [ 4], ARC-c [ 4], and Hellaswag [ 22]) to fully validate the quantized performance\\nofLLAMA3 . For the LoRA-FT methods, we conduct the evaluation on the 5-shot MMLU bench-\\nmark [7] while also validating the aforementioned 5 zero-shot datasets for the LoRA-FT methods.\\nFor the fairness of our evaluation, we uniformly use WikiText2 as the calibration dataset for all\\nquantization methods, with a sample size of 128 and a consistent token sequence length of 2048.\\nFurthermore, for quantization methods requiring channel-wise grouping, we adopt a block size of\\n128 to balance performance and inference efficiency, which is a common practice in existing works.\\n4https://github.com/IST-DASLab/gptq ,https://github.com/mit-han-lab/llm-awq ,https:\\n//github.com/mit-han-lab/smoothquant ,https://github.com/Cornell-RelaxML/QuIP ,https:\\n//github.com/hahnyuan/PB-LLM ,https://github.com/Aaronhuang-778/BiLLM ,https://github.\\ncom/artidoro/qlora ,https://github.com/htqin/IR-QLoRA\\n3', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='c550f03d-768a-4a49-b3d7-2652d2fec2c7', embedding=None, metadata={'page_label': '4', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Flask_Server\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-09', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 2: Evaluation results of post-training quantization on LL AMA3-70B model\\nMethod #W #A #GPPL↓ CommonSenseQA ↑\\nWikiText2 C4 PTB PIQA ARC-e ARC-c HellaSwag Wino Avg.\\nLLAMA3 16 16 - 2.9 6.9 8.2 82.4 86.9 60.3 66.4 80.6 75.3\\nRTN4 16 128 3.6 8.9 9.1 82.3 85.2 58.4 65.6 79.8 74.3\\n3 16 128 11.8 22.0 26.3 64.2 48.9 25.1 41.1 60.5 48.0\\n2 16 128 4.6E5 4.7E5 3.8E5 53.2 23.9 22.1 25.8 53.0 35.6\\nGPTQ4 16 128 3.3 6.9 8.3 82.9 86.3 58.4 66.1 80.7 74.9\\n3 16 128 5.2 10.5 9.7 80.6 79.6 52.1 63.5 77.1 70.6\\n2 16 128 11.9 22.8 31.6 62.7 38.9 24.6 41.0 59.9 45.4\\nAWQ4 16 128 3.3 7.0 8.3 82.7 86.3 59.0 65.7 80.9 74.9\\n3 16 128 4.8 8.0 9.0 81.4 84.7 58.0 63.5 78.6 73.2\\n2 16 128 1.7E6 1.4E6 1.5E6 52.2 25.5 23.1 25.6 52.3 35.7\\nQuIP4 16 - 3.4 7.1 8.4 82.5 86.0 58.7 65.7 79.7 74.5\\n3 16 - 4.7 8.0 8.9 82.3 83.3 54.9 63.9 78.4 72.5\\n2 16 - 13.0 22.2 24.9 65.3 48.9 26.5 40.9 61.7 48.7\\nPB-LLM2 16 128 11.6 34.5 27.2 65.2 40.6 25.1 42.7 56.4 46.0\\n1.7 16 128 18.6 65.2 55.9 56.5 49.9 25.8 34.9 53.1 44.1\\nBiLLM 1.1 16 128 17.1 77.7 54.2 58.2 46.4 25.1 37.5 53.6 44.2\\nSmoothQuant8 8 - 2.9 6.9 8.2 82.2 86.9 60.2 66.3 80.7 75.3\\n6 6 - 2.9 6.9 8.2 82.4 87.0 59.9 66.1 80.6 75.2\\n4 4 - 9.6 16.9 17.7 76.9 75.8 43.5 52.9 58.9 61.6\\n2.2 Track1: Post-Training Quantization\\nAs shown in Table 1 and Table 2, we provide the performance of low-bit LLAMA3 -8B and LLAMA3 -\\n70B with 8 different PTQ methods, respectively, covering a wide bit-width spectrum from 1 to 8-bit.\\nAmong them, Round-To-Nearest (RTN) is a vanilla rounding quantization method. GPTQ [ 6] is\\ncurrently one of the most efficient and effective weight-only quantization methods, which utilizes\\nerror compensation in quantization. But under 2-3 bits, GPTQ causes severe accuracy collapse when\\nquantized LLAMA3 . AWQ [ 10] adopts an anomaly channel suppression approach to reduce the\\ndifficulty of weight quantization, and QuIP [ 2] ensures the incoherence between weights and Hessian\\nby optimizing matrix computation. Both of them can keep LLAMA3 ’s capability at 3-bit and even\\npush the 2-bit quantization to promising.\\nThe recent emergence of binarized LLM quantization methods has realized ultra-low bit-width LLM\\nweight compression. PB-LLM [ 16] employs a mixed-precision quantization strategy, retaining a\\nsmall portion of significant weight full-precision while quantizing the majority of weights to 1-bit.\\nDB-LLM [ 3] achieves efficient LLM compression through double binarization weight splitting\\nand proposes a deviation-aware distillation strategy to further enhance 2-bit LLM performance.\\nBiLLM [ 9] further pushes the LLM quantization boundary to as low as 1.1-bit through residual\\napproximation of salient weights and grouped quantization of non-salient weights. These LLM\\nquantization methods specially designed for ultra-low bit-width can achieve higher accuracy of\\nquantized LLAMA3 -8B at⩽2-bit, far outperforms methods like GPTQ, AWQ, and QuIP under\\n2-bit (even 3-bit some cases).\\nWe also perform LLAMA3 evaluation on quantized activations via SmoothQuant [ 20], which moves\\nthe quantization difficulty offline from activations to weights to smooth out activation outliers. Our\\nevaluation shows that SmoothQuant can retain the accuracy of LLAMA3 with 8- and 6-bit weights\\nand activations, but faces collapse at 4-bit.\\nMoreover, we find that the LLAMA3 -70B model shows significant robustness for various quantization\\nmethods, even in ultra-low bit-width.\\n4', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='5ef3af84-9336-40bb-8050-b6730d85da56', embedding=None, metadata={'page_label': '5', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Flask_Server\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-09', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Table 3: LoRA-FT on LL AMA3-8B with Alpaca dataset\\nMethod #WMMLU ↑ CommonSenseQA ↑\\nHums. STEM Social Other Avg. PIQA ARC-e ARC-c HellaSwag Wino Avg.\\nLLAMA3 16 59.0 55.3 76.0 71.5 64.8 79.9 80.1 50.4 60.2 72.8 68.6\\nNormalFloat 4 56.8 52.9 73.6 69.4 62.5 78.6 78.5 46.2 58.8 74.3 67.3\\nQLoRA 4 50.3 49.3 65.8 64.2 56.7 76.6 74.8 45.0 59.4 67.0 64.5\\nIR-QLoRA 4 52.2 49.0 66.5 63.1 57.2 76.3 74.3 45.3 59.1 69.5 64.9\\n2.3 Track2: LoRA-FineTuning Quantization\\nExcept for the PTQ methods, we also provide the performance of 4-bit LLAMA3 -8B with 2 different\\nLoRA-FT quantization methods as shown in Table 3, including QLoRA [5] and IR-QLoRA [13].\\nOn the MMLU dataset, the most notable observation with LLAMA3 -8B under LoRA-FT quantization\\nis that low-rank finetuning on the Alpaca [ 17] dataset not only cannot compensate for the errors\\nintroduced by quantization, even making the degradation more severe. Specifically, various LoRA-FT\\nquantization methods obtain worse performance quantized LLAMA3 under 4-bit compared with their\\n4-bit counterparts without LoRA-FT. This is in stark contrast to similar phenomena on LLAMA1\\nandLLAMA2 , where, for the front one, the 4-bit low-rank finetuned quantized versions could\\neven easily surpass the original FP16 counterpart on MMLU. According to our intuitive analysis,\\nthe main reason for this phenomenon is due to LLAMA3 ’s strong performance brought by its\\nmassive pre-scale training, which means the performance loss from the original model’s quantization\\ncannot be compensated for by finetuning on a tiny set of data with low-rank parameters (which can\\nbe seen as a subset of the original model [ 8,5]). Despite the significant drop from quantization\\nthat cannot be compensated by finetuning, 4-bit LoRA-FT quantized LLAMA3 -8B significantly\\noutperforms LLAMA1 -7B and LLAMA2 -7B under various quantization methods. For instance, with\\nthe QLoRA method, 4-bit LLAMA3 -8B has an average accuracy of 57.0 (FP16: 64.8), exceeding\\n4-bit LLAMA1 -7B’s 38.4 (FP16: 34.6) by 18.6, and surpassing 4-bit LLAMA2 -7B’s 43.9 (FP16:\\n45.5) by 13.1 [ 21,13]. This implies that a new LoRA-FT quantization paradigm is needed in the era\\nof LL AMA3.\\nA similar phenomenon occurs with the CommonSenseQA benchmark. Compared to the 4-bit\\ncounterparts without LoRA-FT, the performance of the models fine-tuned using QLoRA and IR-\\nQLoRA also declined ( e.g.QLoRA 2.8% vs IR-QLoRA 2.4% on average). This further demonstrates\\nthe strength of using high-quality datasets in LLAMA3 , as the general dataset Alpaca does not\\ncontribute to the model’s performance in other tasks.\\n3 Conclusion\\nMeta’s recently released LLAMA3 models have rapidly become the most powerful LLM series, cap-\\nturing significant interest from researchers. Building on this momentum, our study aims to thoroughly\\nevaluate the performance of LLAMA3 across a variety of low-bit quantization techniques, including\\npost-training quantization and LoRA-finetuning quantization. Our goal is to assess the boundaries\\nof its capabilities in scenarios with limited resources by leveraging existing LLM quantization tech-\\nnologies. Our findings indicate that while LLAMA3 still demonstrates superior performance after\\nquantization, the performance degradation associated with quantization is significant and can even\\nlead to larger declines in many cases. This discovery highlights the potential challenges of deploying\\nLLAMA3 in resource-constrained environments and underscores the ample room for growth and\\nimprovement within the context of low-bit quantization. The empirical insights from our research are\\nexpected to be valuable for the development of future LLM quantization techniques, especially in\\nterms of narrowing the performance gap with the original models. By addressing the performance\\ndegradation caused by low-bit quantization, we anticipate that subsequent quantization paradigms\\nwill enable LLMs to achieve stronger capabilities at a lower computational cost, ultimately driving\\nthe progress of generative artificial intelligence, as represented by LLMs, to new heights.\\n5', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='0285983a-2143-473e-aa63-e8d07288951f', embedding=None, metadata={'page_label': '6', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Flask_Server\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-09', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References\\n[1]Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys-\\nical commonsense in natural language. In Proceedings of the AAAI conference on artificial\\nintelligence , volume 34, pages 7432–7439, 2020.\\n[2]Jerry Chee, Yaohui Cai, V olodymyr Kuleshov, and Christopher M De Sa. Quip: 2-bit quantiza-\\ntion of large language models with guarantees. Advances in Neural Information Processing\\nSystems , 36, 2024.\\n[3]Hong Chen, Chengtao Lv, Liang Ding, Haotong Qin, Xiabin Zhou, Yifu Ding, Xuebo Liu, Min\\nZhang, Jinyang Guo, Xianglong Liu, et al. Db-llm: Accurate dual-binarization for efficient llms.\\narXiv preprint arXiv:2402.11960 , 2024.\\n[4]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\\nchallenge. arXiv preprint arXiv:1803.05457 , 2018.\\n[5]Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient\\nfinetuning of quantized llms. Advances in Neural Information Processing Systems , 36, 2024.\\n[6]Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.\\n[7]Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\\narXiv:2009.03300 , 2020.\\n[8]Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu\\nChen, et al. Lora: Low-rank adaptation of large language models. In International Conference\\non Learning Representations , 2021.\\n[9]Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele\\nMagno, and Xiaojuan Qi. Billm: Pushing the limit of post-training quantization for llms. arXiv\\npreprint arXiv:2402.04291 , 2024.\\n[10] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq:\\nActivation-aware weight quantization for llm compression and acceleration. arXiv preprint\\narXiv:2306.00978 , 2023.\\n[11] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark\\nFerguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating predicate\\nargument structure. In Human Language Technology: Proceedings of a Workshop held at\\nPlainsboro, New Jersey, March 8-11, 1994 , 1994.\\n[12] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\\nmodels. arXiv preprint arXiv:1609.07843 , 2016.\\n[13] Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xi-\\nanglong Liu, and Michele Magno. Accurate lora-finetuning quantization of llms via information\\nretention. arXiv preprint arXiv:2402.05445 , 2024.\\n[14] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\\ntext-to-text transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.\\n[15] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\\nadversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106,\\n2021.\\n[16] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. Pb-llm: Partially binarized large\\nlanguage models. arXiv preprint arXiv:2310.00034 , 2023.\\n6', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
       " Document(id_='83d25da0-301e-4bc7-ac62-f166bce3df78', embedding=None, metadata={'page_label': '7', 'file_name': '2404.14047v1.pdf', 'file_path': 'd:\\\\Main_Projects\\\\Flask_Server\\\\documents\\\\2404.14047v1.pdf', 'file_type': 'application/pdf', 'file_size': 266223, 'creation_date': '2024-08-09', 'last_modified_date': '2024-07-21'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='[17] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\\nhttps://github.com/tatsu-lab/stanford_alpaca , 2023.\\n[18] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\\nand efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\\n[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems , 30, 2017.\\n[20] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.\\nSmoothquant: Accurate and efficient post-training quantization for large language models.\\nInInternational Conference on Machine Learning , pages 38087–38099. PMLR, 2023.\\n[21] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen,\\nXiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large\\nlanguage models. arXiv preprint arXiv:2309.14717 , 2023.\\n[22] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.\\n7', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader,Document\n",
    "documents = SimpleDirectoryReader('./documents').load_data()\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama_index.core.schema.Document"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '0857b3fa-8523-4730-92a3-dc2a74475c0c',\n",
       "  'text': 'How Good Are Low-bit Quantized LLAMA3 Models?\\nAn Empirical Study\\nWei Huang∗\\nThe University of Hong Kong\\nweih@connect.hku.hkXudong Ma∗\\nBeihang University\\nmacaronlin@buaa.edu.cn\\nHaotong Qin†\\nETH Zurich\\nhaotong.qin@pbl.ee.ethz.chXingyu Zheng\\nBeihang University\\nxingyuzheng@buaa.edu.cn\\nChengtao Lv\\nBeihang University\\nlvchengtao@buaa.edu.cnHong Chen\\nBeihang University\\n18373205@buaa.edu.cnJie Luo\\nBeihang University\\nluojie@buaa.edu.cn\\nXiaojuan Qi\\nThe University of Hong Kong\\nxjqi@eee.hku.hkXianglong Liu\\nBeihang University\\nxlliu@buaa.edu.cnMichele Magno\\nETH Zurich\\nmichele.magno@pbl.ee.ethz.ch\\nAbstract\\nMeta’s LLAMA family has become one of the most powerful open-source Large\\nLanguage Model (LLM) series. Notably, LLAMA3 models have recently been\\nreleased and achieve impressive performance across various with super-large scale\\npre-training on over 15T tokens of data. Given the wide application of low-\\nbit quantization for LLMs in resource-limited scenarios, we explore LLAMA3 ’s\\ncapabilities when quantized to low bit-width. This exploration holds the potential to\\nunveil new insights and challenges for low-bit quantization of LLAMA3 and other\\nforthcoming LLMs, especially in addressing performance degradation problems\\nthat suffer in LLM compression. Specifically, we evaluate the 10 existing post-\\ntraining quantization and LoRA-finetuning methods of LLAMA3 on 1-8 bits\\nand diverse datasets to comprehensively reveal LLAMA3 ’s low-bit quantization\\nperformance. Our experiment results indicate that LLAMA3 still suffers non-\\nnegligent degradation in these scenarios, especially in ultra-low bit-width. This\\nhighlights the significant performance gap under low bit-width that needs to be\\nbridged in future developments. We expect that this empirical study will prove\\nvaluable in advancing future models, pushing the LLMs to lower bit-width with\\nhigher accuracy for being practical. Our project is released on https://github.\\ncom/Macaronlin/LLaMA3-Quantization and quantized LLAMA3 models are\\nreleased in https://huggingface.co/LLMQ .\\n1 Introduction\\nLaunched by Meta in February 2023, the LLaMA [ 18] series2represents a breakthrough in autore-\\ngressive large language models (LLMs) using the Transformer [ 19] architecture. Right from its first\\n∗Equal Contribution.†Corresponding Author.\\n2https://llama.meta.comarXiv:2404.14047v1  [cs.LG]  22 Apr 2024'},\n",
       " {'id': '13e77c14-9716-4237-b63c-45874d6dce24',\n",
       "  'text': 'Evaluated LLMsLLaMA3-8BLLaMA3-70B1QuantizationMethodsRTN2GPTQAWQSmoothQuantPB-LLMBiLLMQuIPDB-LLMQLoRAIR-QLoRAEvaluationDatasetsWikiText2C4PTBPIQAARC-eARC-cHellaSwag3WinograndePerplexity↓CommonSenseQA↑Post-Training QuantizationLoRA-FinetuningHumanitiesSTEMSocialOtherMMLU↑Figure 1: The overview of our empirical study\\nversion, with 13 billion parameters, it managed to outperform the much larger, closed-source GPT-3\\nmodel which boasts 175 billion parameters. On April 18, 2024, Meta introduced the LLAMA3\\nmodel, offering configurations of 8 billion and 70 billion parameters. Thanks to extensive pre-training\\non more than 15 trillion data tokens, the LLAMA3 models3have achieved state-of-the-art (SOTA)\\nperformance across a broad range of tasks, establishing the LLaMA family as among the finest\\nopen-source LLMs available for a wide variety of applications and deployment scenarios.\\nDespite their impressive performance, deploying LLAMA3 models still poses significant challenges\\ndue to resource limitations in many scenarios. Fortunately, low-bit quantization has emerged as one\\nof the most popular techniques for compressing LLMs. This technique reduces the memory and\\ncomputational requirements of LLMs during inference, enabling them to run on resource-limited\\ndevices. Addressing the performance drop that occurs after compression is a major concern for\\ncurrent LLM quantization approaches. While numerous low-bit quantization methods have been\\nproposed, their evaluations have primarily focused on the earlier and less capable LLaMA models\\n(LLAMA1 andLLAMA2 ). Thus, LLAMA3 presents a new opportunity for the LLM community\\nto assess the performance of quantization on cutting-edge LLMs and to understand the strengths\\nand limitations of existing methods. In this empirical study, our aim is to analyze the capability of\\nLLAMA3 to handle the challenges associated with degradation due to quantization.\\nOur study sets out two primary technology tracks for quantizing LLMs: Post-Training Quantization\\n(PTQ) and LoRA-FineTuning (LoRA-FT) quantization, with the aim of providing a comprehensive\\nevaluation of the LLAMA3 models’ quantization. We explore a range of cutting-edge quantization\\nmethods across technical tracks (RTN, GPTQ [ 6], AWQ [ 10], SmoothQuant [ 20], PB-LLM [ 16],\\nQuIP [ 2], DB-LLM [ 3], and BiLLM [ 9] for PTQ; QLoRA [ 5] and IR-QLoRA [ 13] for LoRA-FT),\\ncovering a wide spectrum from 1 to 8 bits and utilizing a diverse array of evaluation datasets, including\\nWikiText2, C4, PTB, CommonSenseQA datasets (PIQA, ARC-e, ARC-c, HellaSwag, Winogrande),\\nand MMLU benchmark. The overview of our study is presented as Figure 1. These evaluations\\nassess the capabilities and limits of the LLAMA3 model under current LLM quantization techniques\\nand serve as a source of inspiration for the design of future LLM quantization methods. The choice\\nto focus specifically on the LLAMA3 model is motivated by its superior performance among all\\ncurrent open-source instruction-tuned LLMs across a variety of datasets3, including 5-shot MMLU,\\n0-shot GPQA, 0-shot HumanEval, 8-shot CoT GSM-8K, and 4-shot CoT MATH. Furthermore, we\\nhave made our project and the quantized models available to the public on https://github.com/\\nMacaronlin/LLaMA3-Quantization andhttps://huggingface.co/LLMQ , respectively. This\\nnot only aids in advancing the research within the LLM quantization community but also facilitates a\\nbroader understanding and application of effective quantization techniques.\\n2 Empirical Evaluation\\n2.1 Experiment Settings\\nEvaluated LLMs. We obtain the pre-trained LLAMA3 -8B and -70B through the official repository3.\\nQuantization methods. To evaluate the performance of low-bit quantized LLAMA3 , we select\\nrepresentative LLM quantization methods with extensive influence and functionality, including 8\\n3https://github.com/meta-llama/llama3\\n2'},\n",
       " {'id': 'afe6794c-17fb-412e-864b-d6dea79c137b',\n",
       "  'text': 'Table 1: Evaluation results of post-training quantization on LL AMA3-8B model\\nMethod #W #A #GPPL↓ CommonSenseQA ↑\\nWikiText2 C4 PTB PIQA ARC-e ARC-c HellaSwag Wino Avg.\\nLLAMA3 16 16 - 6.1 9.2 10.6 79.9 80.1 50.4 60.2 72.8 68.6\\nRTN4 16 128 8.5 13.4 14.5 76.6 70.1 45.0 56.8 71.0 63.9\\n3 16 128 27.9 1.1e2 95.6 62.3 32.1 22.5 29.1 54.7 40.2\\n2 16 128 1.9E3 2.5E4 1.8E4 53.1 24.8 22.1 26.9 53.1 36.0\\n8 16 - 6.2 9.5 11.2 79.7 80.8 50.4 60.1 73.4 68.9\\n4 16 - 8.7 14.0 14.9 75.0 68.2 39.4 56.0 69.0 61.5\\n3 16 - 2.2E3 5.6E2 2.0E3 56.2 31.1 20.0 27.5 53.1 35.6\\n2 16 - 2.7E6 7.4E6 3.1E6 53.1 24.7 21.9 25.6 51.1 35.3\\nGPTQ4 16 128 6.5 10.4 11.0 78.4 78.8 47.7 59.0 72.6 67.3\\n3 16 128 8.2 13.7 15.2 74.9 70.5 37.7 54.3 71.1 61.7\\n2 16 128 2.1E2 4.1E4 9.1E2 53.9 28.8 19.9 27.7 50.5 36.2\\n8 16 - 6.1 9.4 10.6 79.8 80.1 50.2 60.2 72.8 68.6\\n4 16 - 7.0 11.8 14.4 76.8 74.3 42.4 57.4 72.8 64.8\\n3 16 - 13.0 45.9 37.0 60.8 38.8 22.3 41.8 60.9 44.9\\n2 16 - 5.7E4 1.0E5 2.7E5 52.8 25.0 20.5 26.6 49.6 34.9\\nAWQ4 16 128 6.6 9.4 11.1 79.1 79.7 49.3 59.1 74.0 68.2\\n3 16 128 8.2 11.6 13.2 77.7 74.0 43.2 55.1 72.1 64.4\\n2 16 128 1.7E6 2.1E6 1.8E6 52.4 24.2 21.5 25.6 50.7 34.9\\n8 16 - 6.1 8.9 10.6 79.6 80.3 50.5 60.2 72.8 68.7\\n4 16 - 7.1 10.1 11.8 78.3 77.6 48.3 58.6 72.5 67.0\\n3 16 - 12.8 16.8 24.0 71.9 66.7 35.1 50.7 64.7 57.8\\n2 16 - 8.2E5 8.1E5 9.0E5 55.2 25.2 21.3 25.4 50.4 35.5\\nQuIP4 16 - 6.5 11.1 9.5 78.2 78.2 47.4 58.6 73.2 67.1\\n3 16 - 7.5 11.3 12.6 76.8 72.9 41.0 55.4 72.5 63.7\\n2 16 - 85.1 1.3E2 1.8E2 52.9 29.0 21.3 29.2 51.7 36.8\\nDB-LLM 2 16 128 13.6 19.2 23.8 68.9 59.1 28.2 42.1 60.4 51.8\\nPB-LLM2 16 128 24.7 79.2 65.6 57.0 37.8 17.2 29.8 52.5 38.8\\n1.7 16 128 41.8 2.6E2 1.2E2 52.5 31.7 17.5 27.7 50.4 36.0\\nBiLLM 1.1 16 128 28.3 2.9E2 94.7 56.1 36.0 17.7 28.9 51.0 37.9\\nSmoothQuant8 8 - 6.3 9.2 10.8 79.5 79.7 49.0 60.0 73.2 68.3\\n6 6 - 7.7 11.8 12.5 76.8 75.5 45.0 56.9 69.0 64.6\\n4 4 - 4.3E3 4.0E3 3.6E3 54.6 26.3 20.0 26.4 50.3 35.5\\nPTQ methods and 2 LoRA-FT methods. The implementations of our evaluated quantization methods\\nfollow their open-source repositories4. We also used eight NVIDIA A800 with 80GB GPU memory\\nfor quantitative evaluation.\\nEvaluation datasets. For the PTQ methods, we evaluate quantized LLAMA3 on the WikiText2 [ 12],\\nPTB [ 11], and a portion of the C4 dataset [ 14], using Perplexity (PPL) as the evaluation metric.\\nSubsequently, we further conduct experiments on five zero-shot evaluation tasks (PIQA [ 1], Wino-\\ngrande [ 15], ARC-e [ 4], ARC-c [ 4], and Hellaswag [ 22]) to fully validate the quantized performance\\nofLLAMA3 . For the LoRA-FT methods, we conduct the evaluation on the 5-shot MMLU bench-\\nmark [7] while also validating the aforementioned 5 zero-shot datasets for the LoRA-FT methods.\\nFor the fairness of our evaluation, we uniformly use WikiText2 as the calibration dataset for all\\nquantization methods, with a sample size of 128 and a consistent token sequence length of 2048.\\nFurthermore, for quantization methods requiring channel-wise grouping, we adopt a block size of\\n128 to balance performance and inference efficiency, which is a common practice in existing works.\\n4https://github.com/IST-DASLab/gptq ,https://github.com/mit-han-lab/llm-awq ,https:\\n//github.com/mit-han-lab/smoothquant ,https://github.com/Cornell-RelaxML/QuIP ,https:\\n//github.com/hahnyuan/PB-LLM ,https://github.com/Aaronhuang-778/BiLLM ,https://github.\\ncom/artidoro/qlora ,https://github.com/htqin/IR-QLoRA\\n3'},\n",
       " {'id': 'c550f03d-768a-4a49-b3d7-2652d2fec2c7',\n",
       "  'text': 'Table 2: Evaluation results of post-training quantization on LL AMA3-70B model\\nMethod #W #A #GPPL↓ CommonSenseQA ↑\\nWikiText2 C4 PTB PIQA ARC-e ARC-c HellaSwag Wino Avg.\\nLLAMA3 16 16 - 2.9 6.9 8.2 82.4 86.9 60.3 66.4 80.6 75.3\\nRTN4 16 128 3.6 8.9 9.1 82.3 85.2 58.4 65.6 79.8 74.3\\n3 16 128 11.8 22.0 26.3 64.2 48.9 25.1 41.1 60.5 48.0\\n2 16 128 4.6E5 4.7E5 3.8E5 53.2 23.9 22.1 25.8 53.0 35.6\\nGPTQ4 16 128 3.3 6.9 8.3 82.9 86.3 58.4 66.1 80.7 74.9\\n3 16 128 5.2 10.5 9.7 80.6 79.6 52.1 63.5 77.1 70.6\\n2 16 128 11.9 22.8 31.6 62.7 38.9 24.6 41.0 59.9 45.4\\nAWQ4 16 128 3.3 7.0 8.3 82.7 86.3 59.0 65.7 80.9 74.9\\n3 16 128 4.8 8.0 9.0 81.4 84.7 58.0 63.5 78.6 73.2\\n2 16 128 1.7E6 1.4E6 1.5E6 52.2 25.5 23.1 25.6 52.3 35.7\\nQuIP4 16 - 3.4 7.1 8.4 82.5 86.0 58.7 65.7 79.7 74.5\\n3 16 - 4.7 8.0 8.9 82.3 83.3 54.9 63.9 78.4 72.5\\n2 16 - 13.0 22.2 24.9 65.3 48.9 26.5 40.9 61.7 48.7\\nPB-LLM2 16 128 11.6 34.5 27.2 65.2 40.6 25.1 42.7 56.4 46.0\\n1.7 16 128 18.6 65.2 55.9 56.5 49.9 25.8 34.9 53.1 44.1\\nBiLLM 1.1 16 128 17.1 77.7 54.2 58.2 46.4 25.1 37.5 53.6 44.2\\nSmoothQuant8 8 - 2.9 6.9 8.2 82.2 86.9 60.2 66.3 80.7 75.3\\n6 6 - 2.9 6.9 8.2 82.4 87.0 59.9 66.1 80.6 75.2\\n4 4 - 9.6 16.9 17.7 76.9 75.8 43.5 52.9 58.9 61.6\\n2.2 Track1: Post-Training Quantization\\nAs shown in Table 1 and Table 2, we provide the performance of low-bit LLAMA3 -8B and LLAMA3 -\\n70B with 8 different PTQ methods, respectively, covering a wide bit-width spectrum from 1 to 8-bit.\\nAmong them, Round-To-Nearest (RTN) is a vanilla rounding quantization method. GPTQ [ 6] is\\ncurrently one of the most efficient and effective weight-only quantization methods, which utilizes\\nerror compensation in quantization. But under 2-3 bits, GPTQ causes severe accuracy collapse when\\nquantized LLAMA3 . AWQ [ 10] adopts an anomaly channel suppression approach to reduce the\\ndifficulty of weight quantization, and QuIP [ 2] ensures the incoherence between weights and Hessian\\nby optimizing matrix computation. Both of them can keep LLAMA3 ’s capability at 3-bit and even\\npush the 2-bit quantization to promising.\\nThe recent emergence of binarized LLM quantization methods has realized ultra-low bit-width LLM\\nweight compression. PB-LLM [ 16] employs a mixed-precision quantization strategy, retaining a\\nsmall portion of significant weight full-precision while quantizing the majority of weights to 1-bit.\\nDB-LLM [ 3] achieves efficient LLM compression through double binarization weight splitting\\nand proposes a deviation-aware distillation strategy to further enhance 2-bit LLM performance.\\nBiLLM [ 9] further pushes the LLM quantization boundary to as low as 1.1-bit through residual\\napproximation of salient weights and grouped quantization of non-salient weights. These LLM\\nquantization methods specially designed for ultra-low bit-width can achieve higher accuracy of\\nquantized LLAMA3 -8B at⩽2-bit, far outperforms methods like GPTQ, AWQ, and QuIP under\\n2-bit (even 3-bit some cases).\\nWe also perform LLAMA3 evaluation on quantized activations via SmoothQuant [ 20], which moves\\nthe quantization difficulty offline from activations to weights to smooth out activation outliers. Our\\nevaluation shows that SmoothQuant can retain the accuracy of LLAMA3 with 8- and 6-bit weights\\nand activations, but faces collapse at 4-bit.\\nMoreover, we find that the LLAMA3 -70B model shows significant robustness for various quantization\\nmethods, even in ultra-low bit-width.\\n4'},\n",
       " {'id': '5ef3af84-9336-40bb-8050-b6730d85da56',\n",
       "  'text': 'Table 3: LoRA-FT on LL AMA3-8B with Alpaca dataset\\nMethod #WMMLU ↑ CommonSenseQA ↑\\nHums. STEM Social Other Avg. PIQA ARC-e ARC-c HellaSwag Wino Avg.\\nLLAMA3 16 59.0 55.3 76.0 71.5 64.8 79.9 80.1 50.4 60.2 72.8 68.6\\nNormalFloat 4 56.8 52.9 73.6 69.4 62.5 78.6 78.5 46.2 58.8 74.3 67.3\\nQLoRA 4 50.3 49.3 65.8 64.2 56.7 76.6 74.8 45.0 59.4 67.0 64.5\\nIR-QLoRA 4 52.2 49.0 66.5 63.1 57.2 76.3 74.3 45.3 59.1 69.5 64.9\\n2.3 Track2: LoRA-FineTuning Quantization\\nExcept for the PTQ methods, we also provide the performance of 4-bit LLAMA3 -8B with 2 different\\nLoRA-FT quantization methods as shown in Table 3, including QLoRA [5] and IR-QLoRA [13].\\nOn the MMLU dataset, the most notable observation with LLAMA3 -8B under LoRA-FT quantization\\nis that low-rank finetuning on the Alpaca [ 17] dataset not only cannot compensate for the errors\\nintroduced by quantization, even making the degradation more severe. Specifically, various LoRA-FT\\nquantization methods obtain worse performance quantized LLAMA3 under 4-bit compared with their\\n4-bit counterparts without LoRA-FT. This is in stark contrast to similar phenomena on LLAMA1\\nandLLAMA2 , where, for the front one, the 4-bit low-rank finetuned quantized versions could\\neven easily surpass the original FP16 counterpart on MMLU. According to our intuitive analysis,\\nthe main reason for this phenomenon is due to LLAMA3 ’s strong performance brought by its\\nmassive pre-scale training, which means the performance loss from the original model’s quantization\\ncannot be compensated for by finetuning on a tiny set of data with low-rank parameters (which can\\nbe seen as a subset of the original model [ 8,5]). Despite the significant drop from quantization\\nthat cannot be compensated by finetuning, 4-bit LoRA-FT quantized LLAMA3 -8B significantly\\noutperforms LLAMA1 -7B and LLAMA2 -7B under various quantization methods. For instance, with\\nthe QLoRA method, 4-bit LLAMA3 -8B has an average accuracy of 57.0 (FP16: 64.8), exceeding\\n4-bit LLAMA1 -7B’s 38.4 (FP16: 34.6) by 18.6, and surpassing 4-bit LLAMA2 -7B’s 43.9 (FP16:\\n45.5) by 13.1 [ 21,13]. This implies that a new LoRA-FT quantization paradigm is needed in the era\\nof LL AMA3.\\nA similar phenomenon occurs with the CommonSenseQA benchmark. Compared to the 4-bit\\ncounterparts without LoRA-FT, the performance of the models fine-tuned using QLoRA and IR-\\nQLoRA also declined ( e.g.QLoRA 2.8% vs IR-QLoRA 2.4% on average). This further demonstrates\\nthe strength of using high-quality datasets in LLAMA3 , as the general dataset Alpaca does not\\ncontribute to the model’s performance in other tasks.\\n3 Conclusion\\nMeta’s recently released LLAMA3 models have rapidly become the most powerful LLM series, cap-\\nturing significant interest from researchers. Building on this momentum, our study aims to thoroughly\\nevaluate the performance of LLAMA3 across a variety of low-bit quantization techniques, including\\npost-training quantization and LoRA-finetuning quantization. Our goal is to assess the boundaries\\nof its capabilities in scenarios with limited resources by leveraging existing LLM quantization tech-\\nnologies. Our findings indicate that while LLAMA3 still demonstrates superior performance after\\nquantization, the performance degradation associated with quantization is significant and can even\\nlead to larger declines in many cases. This discovery highlights the potential challenges of deploying\\nLLAMA3 in resource-constrained environments and underscores the ample room for growth and\\nimprovement within the context of low-bit quantization. The empirical insights from our research are\\nexpected to be valuable for the development of future LLM quantization techniques, especially in\\nterms of narrowing the performance gap with the original models. By addressing the performance\\ndegradation caused by low-bit quantization, we anticipate that subsequent quantization paradigms\\nwill enable LLMs to achieve stronger capabilities at a lower computational cost, ultimately driving\\nthe progress of generative artificial intelligence, as represented by LLMs, to new heights.\\n5'},\n",
       " {'id': '0285983a-2143-473e-aa63-e8d07288951f',\n",
       "  'text': 'References\\n[1]Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about phys-\\nical commonsense in natural language. In Proceedings of the AAAI conference on artificial\\nintelligence , volume 34, pages 7432–7439, 2020.\\n[2]Jerry Chee, Yaohui Cai, V olodymyr Kuleshov, and Christopher M De Sa. Quip: 2-bit quantiza-\\ntion of large language models with guarantees. Advances in Neural Information Processing\\nSystems , 36, 2024.\\n[3]Hong Chen, Chengtao Lv, Liang Ding, Haotong Qin, Xiabin Zhou, Yifu Ding, Xuebo Liu, Min\\nZhang, Jinyang Guo, Xianglong Liu, et al. Db-llm: Accurate dual-binarization for efficient llms.\\narXiv preprint arXiv:2402.11960 , 2024.\\n[4]Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\\nchallenge. arXiv preprint arXiv:1803.05457 , 2018.\\n[5]Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient\\nfinetuning of quantized llms. Advances in Neural Information Processing Systems , 36, 2024.\\n[6]Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 , 2022.\\n[7]Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\\narXiv:2009.03300 , 2020.\\n[8]Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu\\nChen, et al. Lora: Low-rank adaptation of large language models. In International Conference\\non Learning Representations , 2021.\\n[9]Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele\\nMagno, and Xiaojuan Qi. Billm: Pushing the limit of post-training quantization for llms. arXiv\\npreprint arXiv:2402.04291 , 2024.\\n[10] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. Awq:\\nActivation-aware weight quantization for llm compression and acceleration. arXiv preprint\\narXiv:2306.00978 , 2023.\\n[11] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark\\nFerguson, Karen Katz, and Britta Schasberger. The penn treebank: Annotating predicate\\nargument structure. In Human Language Technology: Proceedings of a Workshop held at\\nPlainsboro, New Jersey, March 8-11, 1994 , 1994.\\n[12] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\\nmodels. arXiv preprint arXiv:1609.07843 , 2016.\\n[13] Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xi-\\nanglong Liu, and Michele Magno. Accurate lora-finetuning quantization of llms via information\\nretention. arXiv preprint arXiv:2402.05445 , 2024.\\n[14] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\\ntext-to-text transformer. The Journal of Machine Learning Research , 21(1):5485–5551, 2020.\\n[15] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\\nadversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106,\\n2021.\\n[16] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. Pb-llm: Partially binarized large\\nlanguage models. arXiv preprint arXiv:2310.00034 , 2023.\\n6'},\n",
       " {'id': '83d25da0-301e-4bc7-ac62-f166bce3df78',\n",
       "  'text': '[17] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\\nhttps://github.com/tatsu-lab/stanford_alpaca , 2023.\\n[18] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\\nand efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\\n[19] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\\nprocessing systems , 30, 2017.\\n[20] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.\\nSmoothquant: Accurate and efficient post-training quantization for large language models.\\nInInternational Conference on Machine Learning , pages 38087–38099. PMLR, 2023.\\n[21] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhensu Chen,\\nXiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank adaptation of large\\nlanguage models. arXiv preprint arXiv:2309.14717 , 2023.\\n[22] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a\\nmachine really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.\\n7'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def document_to_dict(doc):\n",
    "    if isinstance(doc, Document):\n",
    "        return {\n",
    "            'id': doc.doc_id,  # Use doc_id instead of id_\n",
    "            'text': doc.text,\n",
    "            # You can include other attributes here as neede\n",
    "        }\n",
    "    return doc  # Return as-is if it's not a Document object\n",
    "\n",
    "dict_documents = [document_to_dict(doc) for doc in documents]\n",
    "dict_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How Good Are Low-bit Quantized LLAMA3 Models?\\nAn Empirical Study\\nWei Huang∗\\nThe University of Hong Kong\\nweih@connect.hku.hkXudong Ma∗\\nBeihang University\\nmacaronlin@buaa.edu.cn\\nHaotong Qin†\\nETH Zurich\\nhaotong.qin@pbl.ee.ethz.chXingyu Zheng\\nBeihang University\\nxingyuzheng@buaa.edu.cn\\nChengtao Lv\\nBeihang University\\nlvchengtao@buaa.edu.cnHong Chen\\nBeihang University\\n18373205@buaa.edu.cnJie Luo\\nBeihang University\\nluojie@buaa.edu.cn\\nXiaojuan Qi\\nThe University of Hong Kong\\nxjqi@eee.hku.hkXianglong Liu\\nBeihang University\\nxlliu@buaa.edu.cnMichele Magno\\nETH Zurich\\nmichele.magno@pbl.ee.ethz.ch\\nAbstract\\nMeta’s LLAMA family has become one of the most powerful open-source Large\\nLanguage Model (LLM) series. Notably, LLAMA3 models have recently been\\nreleased and achieve impressive performance across various with super-large scale\\npre-training on over 15T tokens of data. Given the wide application of low-\\nbit quantization for LLMs in resource-limited scenarios, we explore LLAMA3 ’s\\ncapabilities when quantized to low bit-width. This exploration holds the potential to\\nunveil new insights and challenges for low-bit quantization of LLAMA3 and other\\nforthcoming LLMs, especially in addressing performance degradation problems\\nthat suffer in LLM compression. Specifically, we evaluate the 10 existing post-\\ntraining quantization and LoRA-finetuning methods of LLAMA3 on 1-8 bits\\nand diverse datasets to comprehensively reveal LLAMA3 ’s low-bit quantization\\nperformance. Our experiment results indicate that LLAMA3 still suffers non-\\nnegligent degradation in these scenarios, especially in ultra-low bit-width. This\\nhighlights the significant performance gap under low bit-width that needs to be\\nbridged in future developments. We expect that this empirical study will prove\\nvaluable in advancing future models, pushing the LLMs to lower bit-width with\\nhigher accuracy for being practical. Our project is released on https://github.\\ncom/Macaronlin/LLaMA3-Quantization and quantized LLAMA3 models are\\nreleased in https://huggingface.co/LLMQ .\\n1 Introduction\\nLaunched by Meta in February 2023, the LLaMA [ 18] series2represents a breakthrough in autore-\\ngressive large language models (LLMs) using the Transformer [ 19] architecture. Right from its first\\n∗Equal Contribution.†Corresponding Author.\\n2https://llama.meta.comarXiv:2404.14047v1  [cs.LG]  22 Apr 2024'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_documents[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\langchain\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\langchain\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings import get_embeddings\n",
    "\n",
    "embeddings = get_embeddings()\n",
    "\n",
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Commit-Temp\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
